{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e03142a3-65fe-4996-bd2c-8492b0cbc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade transformers accelerate peft bitsandbytes\n",
    "# !pip install datasets\n",
    "# !pip install scikit-learn\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c84169c-2b6e-4f51-8629-e794102997c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72d20ad1-1cba-4ed0-ba7c-c0a7f35e4b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_final = load_dataset(\"glue\", \"mrpc\")\n",
    "labeled_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a91fff0-4aed-4969-ad04-353f935436bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function\n",
    "def preprocess_text(data):\n",
    "    contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n",
    "\n",
    "    def clean_sentence(sentence):\n",
    "        # 1. Remove extra spaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "\n",
    "        # 2. Remove URLs\n",
    "        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', sentence)\n",
    "\n",
    "        # 3. Remove special characters and punctuation (except dots)\n",
    "        sentence = re.sub(r\"[^\\w\\s.]\", '', sentence)\n",
    "\n",
    "        # 4. Remove consecutive dots\n",
    "        sentence = re.sub(r'\\.{3,}', ' ', sentence)\n",
    "\n",
    "        # 5. Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # 6. Normalize contractions\n",
    "        sentence = ' '.join([contractions[word] if word in contractions else word for word in sentence.split()])\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    for eachsent in range(len(data['sentence1'])):\n",
    "        data['sentence1'][eachsent] = clean_sentence(data['sentence1'][eachsent])\n",
    "\n",
    "    for eachsent in range(len(data['sentence2'])):\n",
    "        data['sentence2'][eachsent] = clean_sentence(data['sentence2'][eachsent])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "353439e4-bff2-449b-91e6-7b3e629561fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = labeled_final[\"train\"].map(preprocess_text, batched=True)\n",
    "valid_dataset = labeled_final[\"validation\"].map(preprocess_text, batched=True)\n",
    "test_dataset = labeled_final[\"test\"].map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41529176-7901-4e3f-92ec-017d100e57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train_dataset.select(range(1000))\n",
    "# valid = valid_dataset.select(range(100))\n",
    "# test = test_dataset.select(range(50))\n",
    "#---\n",
    "train = train_dataset\n",
    "valid = valid_dataset\n",
    "test = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56db88df-96b2-4453-8ba4-7ec5a7140209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'amrozi accused his brother whom he called the witness of deliberately distorting his evidence .',\n",
       " 'sentence2': 'referring to him as only the witness amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75c83c-ee1f-42c8-a525-794ec941758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"\"\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e282115-ca7f-42d7-9c66-8936d4261cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8ab16bc4-120a-4a20-8351-cb484cdecc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Incase of quantization\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# Define QLORA configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization for stability\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type (e.g., NormalFloat4)\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Computation type\n",
    ")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # Low-rank size\n",
    "    lora_alpha=32,                # LoRA scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers (specific to LLaMA)\n",
    "    lora_dropout=0.1,             # Dropout for LoRA layers\n",
    "    bias=\"none\",                  # No bias adaptation\n",
    "    task_type=\"SEQ_CLS\"         # Task type: causal language modeling ###'SEQ_CLS' ###CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,  # Explicitly set for binary classification\n",
    "    device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "907d6193-ab73-4d6a-891e-b9045abc9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "# from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "#use the get_peft_model() function to create a PeftModel from the quantized model and configuration.\n",
    "# from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fa0ee1e-7591-4fbc-a9c4-b8a994cd2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ada8ecf5-1ff0-415a-a03c-2fcb2d903607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tokenizer has a pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Padding token added as [PAD].\")\n",
    "\n",
    "# Resize model embeddings if new token is added\n",
    "if tokenizer.pad_token_id is not None and model.get_input_embeddings().num_embeddings != len(tokenizer):\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set pad_token_id in model configuration\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        padding=True,  # Use padding\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "49385a09-7e52-43cc-8e85-31023f147760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1ebcbe0c-8042-4119-bcca-de6a7b7c2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset = test.map(tokenize_function, batched=True)\n",
    "tokenized_valid_dataset = valid.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ddde8c6-a23e-4055-8ca0-d218a10b0079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the necessary features\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(\n",
    "    lambda examples: {\n",
    "        \"input_ids\": examples[\"input_ids\"],\n",
    "        \"attention_mask\": examples[\"attention_mask\"],\n",
    "        \"labels\": examples[\"label\"]\n",
    "    },\n",
    "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\", \"label\"]\n",
    ")\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4359b36e-e45f-4671-a5b3-5a64f494e67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 408\n",
       "})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the necessary features\n",
    "tokenized_valid_dataset = tokenized_valid_dataset.map(\n",
    "    lambda examples: {\n",
    "        \"input_ids\": examples[\"input_ids\"],\n",
    "        \"attention_mask\": examples[\"attention_mask\"],\n",
    "        \"labels\": examples[\"label\"]\n",
    "    },\n",
    "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\", \"label\"]\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2bee2c83-be8b-4f65-9b23-addcd302615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b97ad028-af2f-43f1-88eb-c810d3211d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",         # Output directory for saved models\n",
    "    learning_rate=2e-5,          # Learning rate\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=16, # Training batch size\n",
    "    per_device_eval_batch_size=16, # Evaluation batch size\n",
    "    num_train_epochs=20,            # Number of training epochs\n",
    "    weight_decay=0.1,             # Weight decay\n",
    "    warmup_steps=250,\n",
    "    #save_total_limit=2,            # Save only the 2 most recent models\n",
    "    logging_dir=\"./logs\",          # Log directory\n",
    "    logging_steps=25,              # Log every 50 steps\n",
    "    load_best_model_at_end=True,    # Load the best model at the end of training\n",
    "    eval_strategy=\"epoch\",   # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",         # Change save_strategy to 'epoch' to match eval_strategy\n",
    "    gradient_accumulation_steps=4, # Added gradient accumulation\n",
    "    fp16=True,                       # Enabled mixed precision training\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "81476632-9127-4269-90d2-f7166c116a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,708,032 || all params: 1,237,526,528 || trainable%: 0.1380\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e0a59849-578d-410a-a50e-87d2a18839ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define a metric function\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)  # Get the class with the highest score\n",
    "\n",
    "#     # Calculate accuracy, precision, recall, and F1\n",
    "#     accuracy = accuracy_score(labels, preds)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1\": f1,\n",
    "#     }\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     results = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#     print(f\"Results keys: {results.keys()}\") #add this line\n",
    "\n",
    "#     accuracy = results[\"accuracy\"]\n",
    "#     f1 = results[\"f1\"]\n",
    "#     precision = results.get(\"precision\", None)\n",
    "#     recall = results.get(\"recall\", None)\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"f1\": f1,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6c6a232-9f4c-47b5-ac8c-ef15534bf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a718eb4-8741-4914-b218-e5efa2d8be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,  # Assuming you have a validation set\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "37b7d9dc-66c1-4c75-b4ee-390e94e79dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='638' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 638/1140 13:20 < 10:32, 0.79 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.018300</td>\n",
       "      <td>0.870621</td>\n",
       "      <td>0.531863</td>\n",
       "      <td>0.623274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.743028</td>\n",
       "      <td>0.649510</td>\n",
       "      <td>0.758037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.648401</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.795341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.556485</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.807560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.500381</td>\n",
       "      <td>0.767157</td>\n",
       "      <td>0.846527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.458546</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>0.864238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.467557</td>\n",
       "      <td>0.789216</td>\n",
       "      <td>0.860390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.433250</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.866438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.444877</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.865517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.438286</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.866197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.452037</td>\n",
       "      <td>0.825980</td>\n",
       "      <td>0.877797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=638, training_loss=0.5182935010676847, metrics={'train_runtime': 802.0147, 'train_samples_per_second': 91.47, 'train_steps_per_second': 1.421, 'total_flos': 2.14746842370048e+16, 'train_loss': 0.5182935010676847, 'epoch': 11.0})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f60b92b8-4eb2-4c51-87e5-7f12df0e11ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43325018882751465, 'eval_accuracy': 0.8088235294117647, 'eval_f1': 0.8664383561643836, 'eval_runtime': 2.444, 'eval_samples_per_second': 166.938, 'eval_steps_per_second': 10.638, 'epoch': 11.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe91c6-44bc-463e-9d3e-40971b2b944b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
