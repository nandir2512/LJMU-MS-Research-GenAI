{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e3e925bb48c4f18abda0c58cc1a3387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22e10b34e7144887974042bf3167a35e",
              "IPY_MODEL_a1539fbf120842689195a024d2769233",
              "IPY_MODEL_d7ac6bc05bf2439fa307afdc3030b544"
            ],
            "layout": "IPY_MODEL_62ef53fc317c4facaf565a074aa674ea"
          }
        },
        "22e10b34e7144887974042bf3167a35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55c22e75a8b14aeba90f05f34d6149d2",
            "placeholder": "​",
            "style": "IPY_MODEL_2641c024558f4b9492f94b350cddb269",
            "value": "Map: 100%"
          }
        },
        "a1539fbf120842689195a024d2769233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_264d0422b31a4716b714a3ca25a83312",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7f48dd71b544967a75f1321db59c999",
            "value": 50
          }
        },
        "d7ac6bc05bf2439fa307afdc3030b544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32734d22f117471796fe24eb1a902cda",
            "placeholder": "​",
            "style": "IPY_MODEL_124d904b511843a6b0aef2a5e1f262c1",
            "value": " 50/50 [00:00&lt;00:00, 539.01 examples/s]"
          }
        },
        "62ef53fc317c4facaf565a074aa674ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55c22e75a8b14aeba90f05f34d6149d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2641c024558f4b9492f94b350cddb269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264d0422b31a4716b714a3ca25a83312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f48dd71b544967a75f1321db59c999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32734d22f117471796fe24eb1a902cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124d904b511843a6b0aef2a5e1f262c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s9tRq_bSD2h2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31oKOET_EJkG",
        "outputId": "ffd5861d-f859-4f5c-d91f-8b4486d262f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load PAWS Subsets\n",
        "#labeled_final = load_dataset(\"paws\", \"labeled_final\")\n",
        "\n",
        "#load MRPC Dataset\n",
        "labeled_final = load_dataset(\"glue\", \"mrpc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ6y8ghsEVJN",
        "outputId": "16fb0cc0-e2f9-472b-ec7c-eec38b39a9d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7yEtwmzEbJx",
        "outputId": "fcb78dcf-a616-4c1a-e6c9-6c92f6f6b8ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Access train, test, and validation splits\n",
        "# labeled_final_train = labeled_final[\"train\"]\n",
        "# labeled_final_test = labeled_final[\"test\"]\n",
        "# labeled_final_validation = labeled_final[\"validation\"]"
      ],
      "metadata": {
        "id": "H-5ZdtVYmOHW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define text preprocessing function\n",
        "def preprocess_text(data):\n",
        "  for eachsent in range(len(data['sentence1'])):\n",
        "\n",
        "    # 1. Remove extra spaces\n",
        "    data['sentence1'][eachsent] = re.sub(r'\\s+', ' ', data['sentence1'][eachsent].strip())\n",
        "\n",
        "    # 2. Remove unwanted text (e.g., URLs, special characters, digits)\n",
        "    data['sentence1'][eachsent] = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', data['sentence1'][eachsent])  # Remove URLs\n",
        "    #data['sentence1'][eachsent] = re.sub(r'[^\\w\\s]', '', data['sentence1'][eachsent])  # Remove special characters (punctuation)\n",
        "\n",
        "    # 3. Convert to lowercase\n",
        "    data['sentence1'][eachsent] = data['sentence1'][eachsent].lower()\n",
        "\n",
        "    # 4. Normalize text (e.g., contractions)\n",
        "    contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n",
        "    data['sentence1'][eachsent] = ' '.join([contractions[word] if word in contractions else word for word in data['sentence1'][eachsent].split()])\n",
        "\n",
        "\n",
        "  for eachsent in range(len(data['sentence2'])):\n",
        "\n",
        "    # 1. Remove extra spaces\n",
        "    data['sentence2'][eachsent] = re.sub(r'\\s+', ' ', data['sentence2'][eachsent].strip())\n",
        "\n",
        "    # 2. Remove unwanted text (e.g., URLs, special characters, digits)\n",
        "    data['sentence2'][eachsent] = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', data['sentence2'][eachsent])  # Remove URLs\n",
        "    #data['sentence2'][eachsent] = re.sub(r'[^\\w\\s]', '', data['sentence2'][eachsent])  # Remove special characters (punctuation)\n",
        "\n",
        "    # 3. Convert to lowercase\n",
        "    data['sentence2'][eachsent] = data['sentence2'][eachsent].lower()\n",
        "\n",
        "    # 4. Normalize text (e.g., contractions)\n",
        "    contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n",
        "    data['sentence2'][eachsent] = ' '.join([contractions[word] if word in contractions else word for word in data['sentence2'][eachsent].split()])\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "V96GIDqXlf_M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = labeled_final[\"train\"].map(preprocess_text, batched=True)\n",
        "valid_dataset = labeled_final[\"validation\"].map(preprocess_text, batched=True)\n",
        "test_dataset = labeled_final[\"test\"].map(preprocess_text, batched=True)"
      ],
      "metadata": {
        "id": "cWNdB7aQlnV8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = labeled_final['train'].select(range(500))\n",
        "# test = labeled_final['test'].select(range(50))\n",
        "# valid = labeled_final['validation'].select(range(50))\n",
        "#---\n",
        "\n",
        "train = train_dataset.select(range(1000))\n",
        "valid = valid_dataset.select(range(50))\n",
        "test = test_dataset.select(range(50))\n",
        "#---\n",
        "# train = train_dataset\n",
        "# valid = valid_dataset\n",
        "# test = test_dataset"
      ],
      "metadata": {
        "id": "txQCAfdbEea2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "movzRa15Emlj",
        "outputId": "35e5b875-235b-4197-80c8-a35d84ed173a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'referring to him as only \" the witness \" , amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN= userdata.get('HuggingFace')\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "YIsyFV_1GTEE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "oGIyf_iwHfkn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since LLAMA3 pre-training doesn't have EOS token\n",
        "* Set the pad_token_id to eos_token_id\n",
        "* Set pad token ot eos_token"
      ],
      "metadata": {
        "id": "Mw9Vgwlj3GlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,device_map=\"auto\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZclzrnCqFAvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28edfcf-aaab-442d-a4a8-98b3f611974c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update some model configs\n",
        "* Must use .cache = False as below or it crashes from my experience"
      ],
      "metadata": {
        "id": "pD5boHFW3SZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "e5NlmHCL3VK1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# # Wrap the model with LoRA\n",
        "# model = get_peft_model(model, lora_config)\n",
        "\n",
        "# # Freeze all model parameters except LoRA layers\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Enable gradients for LoRA layers\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"lora\" in name:\n",
        "#         param.requires_grad = True\n",
        "\n",
        "# # Check which parameters require gradients\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name} requires_grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "id": "C8dUI3CiQMfH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "m6ZF1J0VynNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure tokenizer has a pad_token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    print(\"Padding token added as [PAD].\")\n",
        "\n",
        "# Resize model embeddings if new token is added\n",
        "if tokenizer.pad_token_id is not None and model.get_input_embeddings().num_embeddings != len(tokenizer):\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set pad_token_id in model configuration\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"sentence1\"],\n",
        "        examples[\"sentence2\"],\n",
        "        padding=\"max_length\",  # Use padding\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_train_dataset = train.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "LGfkF13QGkat"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tygA1OJIB8x",
        "outputId": "415dff61-4898-4118-b38c-5074be0cff38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset = test.map(tokenize_function, batched=True)\n",
        "tokenized_valid_dataset = valid.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "BQ_9M2DHIc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0e3e925bb48c4f18abda0c58cc1a3387",
            "22e10b34e7144887974042bf3167a35e",
            "a1539fbf120842689195a024d2769233",
            "d7ac6bc05bf2439fa307afdc3030b544",
            "62ef53fc317c4facaf565a074aa674ea",
            "55c22e75a8b14aeba90f05f34d6149d2",
            "2641c024558f4b9492f94b350cddb269",
            "264d0422b31a4716b714a3ca25a83312",
            "c7f48dd71b544967a75f1321db59c999",
            "32734d22f117471796fe24eb1a902cda",
            "124d904b511843a6b0aef2a5e1f262c1"
          ]
        },
        "outputId": "5044b94e-f510-4cab-8736-fd6c3fac0de3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e3e925bb48c4f18abda0c58cc1a3387"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the necessary features\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(\n",
        "    lambda examples: {\n",
        "        \"input_ids\": examples[\"input_ids\"],\n",
        "        \"attention_mask\": examples[\"attention_mask\"],\n",
        "        \"labels\": examples[\"label\"]\n",
        "    },\n",
        "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\", \"label\"]\n",
        ")\n",
        "tokenized_train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZG5RMq9JCUf",
        "outputId": "e97c2c78-1a41-43d7-a5eb-fec3b84556d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the necessary features\n",
        "tokenized_valid_dataset = tokenized_valid_dataset.map(\n",
        "    lambda examples: {\n",
        "        \"input_ids\": examples[\"input_ids\"],\n",
        "        \"attention_mask\": examples[\"attention_mask\"],\n",
        "        \"labels\": examples[\"label\"]\n",
        "    },\n",
        "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\", \"label\"]\n",
        ")\n",
        "\n",
        "tokenized_valid_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCLGlo0HJTGl",
        "outputId": "41089af1-6bf4-4e8f-b83c-6763ceda1d31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Keep only the necessary features\n",
        "# tokenized_test_dataset = tokenized_test_dataset.map(\n",
        "#     lambda examples: {\n",
        "#         \"input_ids\": examples[\"input_ids\"],\n",
        "#         \"attention_mask\": examples[\"attention_mask\"],\n",
        "#         \"labels\": examples[\"label\"]\n",
        "#     },\n",
        "#     remove_columns=[\"id\", \"sentence1\", \"sentence2\", \"label\"]\n",
        "# )\n",
        "\n",
        "tokenized_test_dataset"
      ],
      "metadata": {
        "id": "81b0LcPZJdBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd93733e-b285-43b6-c22f-ee2e1505de96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "CJuMmxT2zMrL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# disable Weights and Biases\n",
        "os.environ['WANDB_DISABLED']=\"true\"\n"
      ],
      "metadata": {
        "id": "70GeXdlTKwN5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_base_model\",         # Output directory for saved models\n",
        "    learning_rate=2e-5,          # Learning rate\n",
        "    per_device_train_batch_size=2, # Training batch size\n",
        "    per_device_eval_batch_size=2, # Evaluation batch size\n",
        "    num_train_epochs=10,            # Number of training epochs\n",
        "    weight_decay=0.01,             # Weight decay\n",
        "    #save_total_limit=2,            # Save only the 2 most recent models\n",
        "    logging_dir=\"./logs\",          # Log directory\n",
        "    logging_steps=50,              # Log every 50 steps\n",
        "    load_best_model_at_end=True,    # Load the best model at the end of training\n",
        "    eval_strategy=\"epoch\",   # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",         # Change save_strategy to 'epoch' to match eval_strategy\n",
        "    gradient_accumulation_steps=4, # Added gradient accumulation\n",
        "    #fp16=False,                       # Enabled mixed precision training\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "HFR3LQtfJihO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ARn_PCD7QboB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Define a metric function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)  # Get the class with the highest score\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "O9Km97MtYGGl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_valid_dataset,  # Assuming you have a validation set\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkJJhz__Jtxt",
        "outputId": "b4c048e2-085e-4447-a982-c89ce8463b5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-799c2d17add1>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "wyt8O7P0PYO5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7rO1PIgHKkRP",
        "outputId": "1ad87b51-15e1-4fca-c72e-2c4ac1ec3a52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 43.06 MiB is free. Process 41897 has 14.70 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 17.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_pre_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2577\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_xla_gradients_synced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 )\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    149\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 43.06 MiB is free. Process 41897 has 14.70 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 17.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)"
      ],
      "metadata": {
        "id": "UP0UkOrJKk06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = trainer.predict(tokenized_test_dataset)\n",
        "metrics = compute_metrics(test_results)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "SjSMUUBBXlKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the test set\n",
        "predictions = test_results.predictions.argmax(-1)  # Predicted labels\n",
        "true_labels = test_results.label_ids               # True labels\n",
        "\n",
        "# Pair predictions with true labels\n",
        "for i in range(len(predictions)):\n",
        "    print(f\"Sentence1: {tokenized_test_dataset[i]['sentence1']}\")\n",
        "    print(f\"Sentence2: {tokenized_test_dataset[i]['sentence2']}\")\n",
        "    print(f\"True Label: {true_labels[i]}, Predicted Label: {predictions[i]}\\n\")"
      ],
      "metadata": {
        "id": "6zcyXI-EYxBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEb0EaCBYxrb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}